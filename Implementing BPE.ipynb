{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca0e9cd",
   "metadata": {},
   "source": [
    "# Implemeting Byte-Pair Encoding (BPE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eca43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "class BPETokenizer:\n",
    "  def __init__(self, text, vocab_size):\n",
    "    self.vocab_size = vocab_size\n",
    "    self.create_initial_vocab(text)\n",
    "\n",
    "  def create_initial_vocab(self, text):\n",
    "    text = text.replace(\" \", \"#\")\n",
    "    text = list(text)\n",
    "    vocab = list(set(text))\n",
    "\n",
    "    text_to_id = {vocab[i]: i for i in range(len(vocab))}\n",
    "    id_to_text = {i: vocab[i] for i in range(len(vocab))}\n",
    "\n",
    "    self.text_to_id = text_to_id\n",
    "    self.id_to_text = id_to_text\n",
    "    self.ids_to_merge = [text_to_id[i] for i in text]\n",
    "\n",
    "  def merge(self, ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "      if ids[i] == pair[0] and i < len(ids) and ids[i + 1] == pair[1]:\n",
    "        newids.append(idx)\n",
    "        i += 2\n",
    "      else:\n",
    "        newids.append(ids[i])\n",
    "        i += 1\n",
    "    return newids\n",
    "\n",
    "  def train(self):\n",
    "    merges = {}\n",
    "    num_merges = self.vocab_size - len(self.text_to_id)\n",
    "\n",
    "    for i in range(num_merges):\n",
    "      bigrams = nltk.ngrams(self.ids_to_merge, 2)\n",
    "      within_word_bigrams = [i for i in bigrams if '#' not in self.id_to_text[i[0]]]\n",
    "      counts = Counter(within_word_bigrams)\n",
    "      candidate = counts.most_common(1)[0][0]\n",
    "      new_id = len(self.id_to_text) + 1\n",
    "      self.ids_to_merge = self.merge(self.ids_to_merge, candidate, new_id)\n",
    "      merges[candidate] = new_id\n",
    "      self.id_to_text[new_id] = self.id_to_text[candidate[0]] + self.id_to_text[candidate[1]]\n",
    "      self.text_to_id[self.id_to_text[candidate[0]] + self.id_to_text[candidate[1]]] = new_id\n",
    "\n",
    "    self.merges = merges\n",
    "    print('Training Complete!')\n",
    "\n",
    "  def decode(self, ids):\n",
    "    return[self.id_to_text[i] for i in ids]\n",
    "\n",
    "  def encode(self, text):\n",
    "    text = text.replace(\" \", \"#\")\n",
    "    text = list(text)\n",
    "    text.append('#')\n",
    "    ids = [self.text_to_id[i] for i in text]\n",
    "    while len(ids) >= 2:\n",
    "      stats = nltk.ngrams(ids, 2)\n",
    "      pair = min(stats, key=lambda i: self.merges.get(i, float(\"inf\")))\n",
    "      if pair not in self.merges:\n",
    "        break\n",
    "      idx = self.merges[pair]\n",
    "      ids = self.merge(ids, pair, idx)\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer(clean_book, 500)\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc4bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode('Romeo, my love.')\n",
    "print(tokens)\n",
    "print(tokenizer.decode(list(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce725fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\") # here they have added a SOS marker instead of an EOS marker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=500, special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annoyingly, the tokenizers library requires text files as input for training, so let's save our text in a file:\n",
    "with open('clean_book.txt', 'w') as outfile:\n",
    "  outfile.write(clean_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train([\"clean_book.txt\"], trainer=trainer) # train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Romeo, my love.\")\n",
    "print(encoding.tokens)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
